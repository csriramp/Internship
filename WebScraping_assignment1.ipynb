{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c3866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>Header Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>æ¥æ¬èª</td>\n",
       "      <td>ja.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EspaÃ±ol</td>\n",
       "      <td>es.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ð ÑÑÑÐºÐ¸Ð¹</td>\n",
       "      <td>ru.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>de.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FranÃ§ais</td>\n",
       "      <td>fr.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>it.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ä¸­æ</td>\n",
       "      <td>zh.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PortuguÃªs</td>\n",
       "      <td>pt.wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ø§ÙØ¹Ø±Ø¨ÙØ©</td>\n",
       "      <td>ar.wikipedia.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Header       Header Link\n",
       "0         English  en.wikipedia.org\n",
       "1       æ¥æ¬èª  ja.wikipedia.org\n",
       "2        EspaÃ±ol  es.wikipedia.org\n",
       "3  Ð ÑÑÑÐºÐ¸Ð¹  ru.wikipedia.org\n",
       "4         Deutsch  de.wikipedia.org\n",
       "5       FranÃ§ais  fr.wikipedia.org\n",
       "6        Italiano  it.wikipedia.org\n",
       "7          ä¸­æ  zh.wikipedia.org\n",
       "8      PortuguÃªs  pt.wikipedia.org\n",
       "9  Ø§ÙØ¹Ø±Ø¨ÙØ©  ar.wikipedia.org"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 1 WIkipedia.org\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://www.wikipedia.org/'\n",
    "\n",
    "def wiki_webscraping(url):\n",
    "    \n",
    "    source=requests.get(url)\n",
    "    soup=BeautifulSoup(source.text,'html.parser')\n",
    "    soup\n",
    "\n",
    "    header_language=[]\n",
    "    link_language=[]\n",
    "\n",
    "    i=1\n",
    "    while i <=10:\n",
    "        for j in  soup.find_all('div',class_='central-featured-lang lang'+str(i)):\n",
    "            header_language.append(j.strong.text)\n",
    "            link_language.append(j.a['href'].strip('/'))\n",
    "        i+=1\n",
    "\n",
    "    wiki_df=pd.DataFrame({'Header':header_language,'Header Link':link_language})\n",
    "    return wiki_df\n",
    "        \n",
    "wiki_webscraping(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf691e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking</th>\n",
       "      <th>Name of Film</th>\n",
       "      <th>Year of Release</th>\n",
       "      <th>Rating of Film</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>2003</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>1993</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>1959</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Vertigo</td>\n",
       "      <td>1958</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>1952</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>1941</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>M</td>\n",
       "      <td>1931</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ranking                                   Name of Film Year of Release  \\\n",
       "0         1                       The Shawshank Redemption            1994   \n",
       "1         2                                  The Godfather            1972   \n",
       "2         3                                The Dark Knight            2008   \n",
       "3         4  The Lord of the Rings: The Return of the King            2003   \n",
       "4         5                               Schindler's List            1993   \n",
       "..      ...                                            ...             ...   \n",
       "95       96                             North by Northwest            1959   \n",
       "96       97                                        Vertigo            1958   \n",
       "97       98                            Singin' in the Rain            1952   \n",
       "98       99                                   Citizen Kane            1941   \n",
       "99      100                                              M            1931   \n",
       "\n",
       "    Rating of Film  \n",
       "0              9.3  \n",
       "1              9.2  \n",
       "2              9.0  \n",
       "3              9.0  \n",
       "4              9.0  \n",
       "..             ...  \n",
       "95             8.3  \n",
       "96             8.3  \n",
       "97             8.3  \n",
       "98             8.3  \n",
       "99             8.3  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2 IMDB top 100 movies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url=['https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc','https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt']\n",
    "\n",
    "def imdb_top(url):\n",
    "    rank=[]\n",
    "    name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    rep={'(':'',')':''}\n",
    "    for i in range(len(url)):\n",
    "        source= requests.get(url[i])\n",
    "        soup=BeautifulSoup(source.text,'html.parser')\n",
    "\n",
    "        for j in soup.find_all('h3',class_='lister-item-header') :\n",
    "            rank.append(j.text.split('\\n')[1].replace('.',''))\n",
    "            name.append(j.text.split('\\n')[2])\n",
    "            year.append(j.find('span',class_='lister-item-year text-muted unbold').text)\n",
    "        for i in soup.find_all('div', class_='inline-block ratings-imdb-rating') :\n",
    "            rating.append(i.text.split('\\n')[2])\n",
    "        for key,value in rep.items():\n",
    "            year = [ s.replace(key,value) for s in year]\n",
    "\n",
    "        rank = [int(s) for s in rank]\n",
    "        rating = [float(s) for s in rating]\n",
    "\n",
    "    imdb_df= pd.DataFrame({'Ranking':rank,'Name of Film':name,'Year of Release':year ,'Rating of Film':rating })\n",
    "    return imdb_df\n",
    "\n",
    "imdb_top(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4382f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking</th>\n",
       "      <th>Name of Movie</th>\n",
       "      <th>Year of Release</th>\n",
       "      <th>Rating of Movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>2003</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Golmaal</td>\n",
       "      <td>1979</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Nayakan</td>\n",
       "      <td>1987</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>2021</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>2006</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Baasha</td>\n",
       "      <td>1995</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Masaan</td>\n",
       "      <td>2015</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Virumandi</td>\n",
       "      <td>2004</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ranking                Name of Movie  Year of Release  Rating of Movie\n",
       "0         1   Rocketry: The Nambi Effect             2022              8.5\n",
       "1         2                   Anbe Sivam             2003              8.4\n",
       "2         3                      Golmaal             1979              8.4\n",
       "3         4                      Nayakan             1987              8.4\n",
       "4         5                     Jai Bhim             2021              8.4\n",
       "..      ...                          ...              ...              ...\n",
       "95       96              Rang De Basanti             2006              8.0\n",
       "96       97  Baahubali 2: The Conclusion             2017              8.0\n",
       "97       98                       Baasha             1995              8.0\n",
       "98       99                       Masaan             2015              8.0\n",
       "99      100                    Virumandi             2004              8.0\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3 IMDB top 100 Indian Movies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url='https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "\n",
    "def imdb_indian_top(url):\n",
    "    \n",
    "    source=requests.get(url)\n",
    "    source\n",
    "\n",
    "    soup=BeautifulSoup(source.text,'html.parser')\n",
    "    soup\n",
    "\n",
    "    rank=[]\n",
    "    name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    rep={'(':'',')':'','\\n':''}\n",
    "\n",
    "    movies = soup.find('tbody',class_='lister-list').find_all('tr')\n",
    "\n",
    "    for i in movies:\n",
    "        rank.append(i.find('td',class_='titleColumn').get_text(strip=True).split('.')[0])\n",
    "        name.append(i.find('td',class_='titleColumn').a.text)\n",
    "        year.append(i.find('td',class_='titleColumn').span.text.strip('()'))\n",
    "        rating.append(i.find('td',class_='ratingColumn imdbRating').strong.text)\n",
    "\n",
    "    # Formatting and slicing for 100 records\n",
    "    rank = [int(s) for s in rank[0:100]]\n",
    "    year = [int(s) for s in year[0:100]]\n",
    "    rating = [float(s) for s in rating[0:100]]\n",
    "    name = name[0:100]\n",
    "    \n",
    "    imdb_df=pd.DataFrame({'Ranking':rank,'Name of Movie':name,'Year of Release':year,'Rating of Movie':rating})\n",
    "    return imdb_df\n",
    "\n",
    "imdb_indian_top(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d670a479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of President</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name of President  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1          Shri Pranab Mukherjee    \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3         DR. A.P.J. Abdul Kalam    \n",
       "4           Shri K. R. Narayanan    \n",
       "5        Dr Shankar Dayal Sharma    \n",
       "6            Shri R Venkataraman    \n",
       "7               Giani Zail Singh    \n",
       "8      Shri Neelam Sanjiva Reddy    \n",
       "9       Dr. Fakhruddin Ali Ahmed    \n",
       "10  Shri Varahagiri Venkata Giri    \n",
       "11              Dr. Zakir Husain    \n",
       "12  Dr. Sarvepalli Radhakrishnan    \n",
       "13           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 4 President Of India \n",
    "\n",
    "url='https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "def president_of_India():\n",
    "    source=requests.get(url)\n",
    "    soup=BeautifulSoup(source.text,'html.parser')\n",
    "    \n",
    "    name=[]\n",
    "    term=[]\n",
    "    \n",
    "    for i in soup.find_all('div',class_=\"presidentListing\") :\n",
    "        name.append(i.h3.text.split('(')[0])\n",
    "        term.append(i.p.text.split(':')[1])\n",
    "    \n",
    "    president_of_India=pd.DataFrame({'Name of President':name,\n",
    "                                     'Term of Office':term})\n",
    "    return president_of_India\n",
    "    \n",
    "president_of_India()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43786816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 'mto' for Men ODI ranking\n",
      " or \n",
      "Enter 'wto' for women ODI ranking : wto\n",
      "\n",
      " Details are scraped for selection :  wto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rankings</th>\n",
       "      <th>Name</th>\n",
       "      <th>No of Matches</th>\n",
       "      <th>Total Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>4,837</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>England</td>\n",
       "      <td>33</td>\n",
       "      <td>4,046</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>35</td>\n",
       "      <td>4,157</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>India</td>\n",
       "      <td>32</td>\n",
       "      <td>3,219</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>1,962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>9</td>\n",
       "      <td>405</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>11</td>\n",
       "      <td>495</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rankings          Name No of Matches Total Points Rating\n",
       "0        1     Australia            29        4,837    167\n",
       "1        2       England            33        4,046    123\n",
       "2        3  South Africa            35        4,157    119\n",
       "3        4         India            32        3,219    101\n",
       "4        5   New Zealand            31        3,019     97\n",
       "5        6   West Indies            30        2,768     92\n",
       "6        7    Bangladesh            12          930     78\n",
       "7        8      Pakistan            30        1,962     65\n",
       "8        9       Ireland             9          405     45\n",
       "9       10     Sri Lanka            11          495     45"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Program for Question 5a and 6a combined 5a(Top 10 Men ODI Teams)  6a(Top 10 Women ODI Teams) , please provide input\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "mto='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "wto='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "mtt='https://www.icc-cricket.com/rankings/mens/team-rankings/test'\n",
    "\n",
    "\n",
    "url_inp=input(\"Enter 'mto' for Men ODI ranking\\n or \\nEnter 'wto' for women ODI ranking : \")\n",
    "\n",
    "print('\\n Details are scraped for selection : ', url_inp)\n",
    "\n",
    "url=[]\n",
    "\n",
    "if url_inp=='mto':\n",
    "    url.append(mto)\n",
    "elif url_inp=='wto':\n",
    "    url.append(wto)\n",
    "elif url_inp=='mtt':\n",
    "    url.append(mtt)\n",
    "\n",
    "\n",
    "def icc_webscrap_team(url):\n",
    "    for i in range(len(url)):\n",
    "        source=requests.get(url[i])\n",
    "        soup=BeautifulSoup(source.text,'html.parser')\n",
    "        \n",
    "        rank=[]\n",
    "        name=[]\n",
    "        matches=[]\n",
    "        points=[]\n",
    "        ratings=[]\n",
    "\n",
    "        rank.append(soup.find('tr',class_='rankings-block__banner').text.split()[0])\n",
    "        name.append(soup.find('span',class_='u-hide-phablet').text)\n",
    "        matches.append(soup.find('tr',class_='rankings-block__banner').text.split()[-3])\n",
    "        points.append(soup.find('tr',class_='rankings-block__banner').text.split()[-2])\n",
    "        ratings.append(soup.find('tr',class_='rankings-block__banner').text.split()[-1])\n",
    "        \n",
    "        for i in soup.find_all('tr',class_='table-body'):\n",
    "            rank.append(i.text.split()[0])\n",
    "            name.append(i.find('span',class_='u-hide-phablet').text)\n",
    "            matches.append(i.text.split()[-3])\n",
    "            points.append(i.text.split()[-2])\n",
    "            ratings.append(i.text.split()[-1])\n",
    "            \n",
    "    icc_team_df=pd.DataFrame({'Rankings':rank[0:10],'Name':name[0:10],'No of Matches':matches[0:10],'Total Points':points[0:10],'Rating':ratings[0:10]})\n",
    "    return icc_team_df\n",
    "\n",
    "\n",
    "icc_webscrap_team(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc923a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any one among \n",
      " 'menbat' for Top10 Men Batsmen \n",
      " 'menbow' for Top10 Men Bowlers \n",
      " 'wombat' for Top10 Women batting \n",
      " 'womall' for Top10 women allrounders :  womall\n",
      "\n",
      " Details are scraped for selection : womall\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank of Player</th>\n",
       "      <th>Name of Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Total Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank of Player    Name of Player Team  Total Rating\n",
       "0               1    Natalie Sciver  ENG           379\n",
       "1               2      Ellyse Perry  AUS           374\n",
       "2               3    Marizanne Kapp   SA           349\n",
       "3               4   Hayley Matthews   WI           339\n",
       "4               5       Amelia Kerr   NZ           336\n",
       "5               6  Ashleigh Gardner  AUS           270\n",
       "6               7     Deepti Sharma  IND           252\n",
       "7               8     Jess Jonassen  AUS           246\n",
       "8               9   Katherine Brunt  ENG           220\n",
       "9              10   Stafanie Taylor   WI           207"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Program for question 5b,5c and 6b,6c (5b-Top 10 men ODI batsmen,5c-Top 10 men ODI bowler , 6b-Top10 women ODI batting,6c-Top 10 Women allrounder) , please give input\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "menbat='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "menbow='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "wombat='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "womall='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "\n",
    "url_inp=input(\"Enter any one among \\n 'menbat' for Top10 Men Batsmen \\n 'menbow' for Top10 Men Bowlers \\n 'wombat' for Top10 Women batting \\n 'womall' for Top10 women allrounders :  \")\n",
    "print('\\n Details are scraped for selection :', url_inp)\n",
    "\n",
    "url=[]\n",
    "\n",
    "if url_inp=='menbat':\n",
    "    url.append(menbat)\n",
    "elif url_inp=='menbow':\n",
    "    url.append(menbow)\n",
    "elif url_inp=='wombat':\n",
    "    url.append(wombat)\n",
    "elif url_inp=='womall':\n",
    "    url.append(womall)\n",
    "    \n",
    "\n",
    "def icc_webscrap_player(url):\n",
    "    for i in range(len(url)):\n",
    "        source=requests.get(url[i])\n",
    "        soup=BeautifulSoup(source.text,'html.parser')\n",
    "\n",
    "        rank=[]\n",
    "        player_name=[]\n",
    "        team=[]\n",
    "        ratings=[]\n",
    "\n",
    "        rank.append(soup.find('tr',class_='rankings-block__banner').text.split()[0])\n",
    "        player_name.append(soup.find('div',class_='rankings-block__banner--name-large').text)\n",
    "        team.append(soup.find('div',class_='rankings-block__banner--nationality').text.strip('\\n'))\n",
    "        ratings.append(soup.find('td',class_='u-text-left').text.strip('\\n')) \n",
    "\n",
    "        for i in soup.find_all('tr',class_='table-body'):\n",
    "            rank.append(i.text.split()[0])\n",
    "            player_name.append(i.find('td',class_='table-body__cell rankings-table__name name').text.strip('\\n'))\n",
    "            team.append(i.find('span',class_='table-body__logo-text').text)\n",
    "            ratings.append(i.find('td',class_='table-body__cell rating').text)\n",
    "\n",
    "        rank=[int(s) for s in rank]\n",
    "        ratings=[int(s) for s in ratings]\n",
    "\n",
    "    icc_player_df=pd.DataFrame({'Rank of Player':rank[0:10],'Name of Player':player_name[0:10],'Team':team[0:10],'Total Rating':ratings[0:10]})\n",
    "    return icc_player_df\n",
    "    \n",
    "icc_webscrap_player(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4a2d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline of news</th>\n",
       "      <th>Time since news released</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Airbnb hosts buy long-abandoned TX house and f...</td>\n",
       "      <td>19 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/texas-couple-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read these 6 books if you want to learn more a...</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/6-books-to-rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This retired couple left the U.S. and bought a...</td>\n",
       "      <td>20 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/retired-couple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hold on before jumping to the conclusion the m...</td>\n",
       "      <td>21 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/testing-junes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. companies are reshoring at a rapid pace. ...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/us-companies-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wall Street analysts name the most well-positi...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/analysts-say-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Energy companies' cash flows could top $1 tril...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/energy-compani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The 10 best U.S. cities for new grads to start...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/best-us-cities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Climate change may bring back wind as the futu...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/how-ocean-ship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Here's how to figure out if you qualify for fe...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/do-you-make-to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mortgage denial rate for Black borrowers is tw...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/black-borrower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NASA is set to launch the Artemis 1 mission on...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/nasas-artemis-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How the U.S. government can keep household deb...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/how-the-us-gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Breakfast sales hold steady as people heading ...</td>\n",
       "      <td>24 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/27/breakfast-sale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A key leader for Meta's metaverse software is ...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/meta-horizon-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNBC in 5 minutes: All the buy, sell and hold ...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/cnbc-in-5-minu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CDC cautiously optimistic monkeypox outbreak m...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/monkeypox-cdc-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What to watch in the markets in the week ahead</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/stocks-face-mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My friend was able to unlock my new Google pho...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/google-pixel-6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Investing Club: The week in review, the week a...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/investing-club...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Student loan forgiveness: It could take 'sever...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/student-loan-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GOP warns Arizona Senate hopeful Masters to ra...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/arizona-senate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Google employees frustrated by Covid outbreaks...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/google-employe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Here's how Biden's student loan cancellation a...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/heres-how-bide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Dr. Oz defends energy industry, which has boos...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/dr-oz-sides-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>These 13 states may tax student loan forgiveness</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/13-states-may-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Why Peloton's partnership with Amazon won't re...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/pelotons-partn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Secret Service returns $286 million in fraudul...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/secret-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4 Takeaways from the Investing Club’s ‘Morning...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/4-takeaways-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Energy is the big winner of the week. Here are...</td>\n",
       "      <td>August 26, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/08/26/energy-is-the-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Headline of news  \\\n",
       "0   Airbnb hosts buy long-abandoned TX house and f...   \n",
       "1   Read these 6 books if you want to learn more a...   \n",
       "2   This retired couple left the U.S. and bought a...   \n",
       "3   Hold on before jumping to the conclusion the m...   \n",
       "4   U.S. companies are reshoring at a rapid pace. ...   \n",
       "5   Wall Street analysts name the most well-positi...   \n",
       "6   Energy companies' cash flows could top $1 tril...   \n",
       "7   The 10 best U.S. cities for new grads to start...   \n",
       "8   Climate change may bring back wind as the futu...   \n",
       "9   Here's how to figure out if you qualify for fe...   \n",
       "10  Mortgage denial rate for Black borrowers is tw...   \n",
       "11  NASA is set to launch the Artemis 1 mission on...   \n",
       "12  How the U.S. government can keep household deb...   \n",
       "13  Breakfast sales hold steady as people heading ...   \n",
       "14  A key leader for Meta's metaverse software is ...   \n",
       "15  CNBC in 5 minutes: All the buy, sell and hold ...   \n",
       "16  CDC cautiously optimistic monkeypox outbreak m...   \n",
       "17     What to watch in the markets in the week ahead   \n",
       "18  My friend was able to unlock my new Google pho...   \n",
       "19  Investing Club: The week in review, the week a...   \n",
       "20  Student loan forgiveness: It could take 'sever...   \n",
       "21  GOP warns Arizona Senate hopeful Masters to ra...   \n",
       "22  Google employees frustrated by Covid outbreaks...   \n",
       "23  Here's how Biden's student loan cancellation a...   \n",
       "24  Dr. Oz defends energy industry, which has boos...   \n",
       "25  These 13 states may tax student loan forgiveness    \n",
       "26  Why Peloton's partnership with Amazon won't re...   \n",
       "27  Secret Service returns $286 million in fraudul...   \n",
       "28  4 Takeaways from the Investing Club’s ‘Morning...   \n",
       "29  Energy is the big winner of the week. Here are...   \n",
       "\n",
       "   Time since news released                                          News Link  \n",
       "0              19 Hours Ago  https://www.cnbc.com/2022/08/27/texas-couple-t...  \n",
       "1              20 Hours Ago  https://www.cnbc.com/2022/08/27/6-books-to-rea...  \n",
       "2              20 Hours Ago  https://www.cnbc.com/2022/08/27/retired-couple...  \n",
       "3              21 Hours Ago  https://www.cnbc.com/2022/08/27/testing-junes-...  \n",
       "4              22 Hours Ago  https://www.cnbc.com/2022/08/27/us-companies-a...  \n",
       "5              22 Hours Ago  https://www.cnbc.com/2022/08/27/analysts-say-b...  \n",
       "6              22 Hours Ago  https://www.cnbc.com/2022/08/27/energy-compani...  \n",
       "7              22 Hours Ago  https://www.cnbc.com/2022/08/27/best-us-cities...  \n",
       "8              22 Hours Ago  https://www.cnbc.com/2022/08/27/how-ocean-ship...  \n",
       "9              22 Hours Ago  https://www.cnbc.com/2022/08/27/do-you-make-to...  \n",
       "10             22 Hours Ago  https://www.cnbc.com/2022/08/27/black-borrower...  \n",
       "11             23 Hours Ago  https://www.cnbc.com/2022/08/27/nasas-artemis-...  \n",
       "12             23 Hours Ago  https://www.cnbc.com/2022/08/27/how-the-us-gov...  \n",
       "13             24 Hours Ago  https://www.cnbc.com/2022/08/27/breakfast-sale...  \n",
       "14          August 26, 2022  https://www.cnbc.com/2022/08/26/meta-horizon-v...  \n",
       "15          August 26, 2022  https://www.cnbc.com/2022/08/26/cnbc-in-5-minu...  \n",
       "16          August 26, 2022  https://www.cnbc.com/2022/08/26/monkeypox-cdc-...  \n",
       "17          August 26, 2022  https://www.cnbc.com/2022/08/26/stocks-face-mo...  \n",
       "18          August 26, 2022  https://www.cnbc.com/2022/08/26/google-pixel-6...  \n",
       "19          August 26, 2022  https://www.cnbc.com/2022/08/26/investing-club...  \n",
       "20          August 26, 2022  https://www.cnbc.com/2022/08/26/student-loan-f...  \n",
       "21          August 26, 2022  https://www.cnbc.com/2022/08/26/arizona-senate...  \n",
       "22          August 26, 2022  https://www.cnbc.com/2022/08/26/google-employe...  \n",
       "23          August 26, 2022  https://www.cnbc.com/2022/08/26/heres-how-bide...  \n",
       "24          August 26, 2022  https://www.cnbc.com/2022/08/26/dr-oz-sides-wi...  \n",
       "25          August 26, 2022  https://www.cnbc.com/2022/08/26/13-states-may-...  \n",
       "26          August 26, 2022  https://www.cnbc.com/2022/08/26/pelotons-partn...  \n",
       "27          August 26, 2022  https://www.cnbc.com/2022/08/26/secret-service...  \n",
       "28          August 26, 2022  https://www.cnbc.com/2022/08/26/4-takeaways-fr...  \n",
       "29          August 26, 2022  https://www.cnbc.com/2022/08/26/energy-is-the-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 7 News headlines from CNBC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "def cnbc_webscraping(url):\n",
    "    source=requests.get(url)\n",
    "    soup=BeautifulSoup(source.text,'html.parser')\n",
    "\n",
    "    data=soup.find('div',class_='undefined LatestNews-isHomePage LatestNews-isIntlHomepage')\n",
    "    \n",
    "    headline=[]\n",
    "    time=[]\n",
    "    link=[]\n",
    "\n",
    "    for i in data.find_all('div',class_='LatestNews-headlineWrapper'):\n",
    "        headline.append(i.find('a',class_='LatestNews-headline').text)\n",
    "        time.append(i.span.text)\n",
    "        link.append(i.find('a',class_='LatestNews-headline')['href'])\n",
    "\n",
    "    cnbc_df=pd.DataFrame({'Headline of news':headline,'Time since news released':time,'News Link':link})\n",
    "    return cnbc_df\n",
    "\n",
    "cnbc_webscraping(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb61138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published URL</th>\n",
       "      <th>Published Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>[Silver,  David,  Singh,  Satinder,  Precup,  ...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>October 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>[Evans,  Richard,  Bošnjak,  Matko and 5 more]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>October 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>[Prakken,  Henry,  Sartor,  Giovanni ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>October 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>[Boden,  Margaret A. ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>August 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>[Lemaignan,  Séverin,  Warnier,  Mathieu and 3...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>June 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>[Miller,  Tim ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>February 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>[Evans,  Richard,  Hernández-Orallo,  José and...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>April 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>[Sharon,  Guni,  Stern,  Roni,  Felner,  Ariel...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>February 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>[Sutton,  Richard S.,  Precup,  Doina,  Singh,...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>August 1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>[Bard,  Nolan,  Foerster,  Jakob N. and 13 more]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>March 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>[van der Waa,  Jasper,  Nieuwburg,  Elisabeth,...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>February 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>[Bench-Capon,  T.J.M.,  Dunne,  Paul E. ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>October 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>[Bošanský,  Branislav,  Lisý,  Viliam and 3 more]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>August 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>[Luo,  Wenhan,  Xing,  Junliang and 4 more]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>April 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>[Blum,  Avrim L.,  Langley,  Pat ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>December 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>[Arora,  Saurabh,  Doshi,  Prashant ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>August 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>[Aas,  Kjersti,  Jullum,  Martin,  Løland,  An...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>September 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>[Kliegr,  Tomáš,  Bahník,  Štěpán,  Fürnkranz,...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>June 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>[Pereira,  Gonçalo,  Prada,  Rui,  Santos,  Pe...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>December 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>[Riveiro,  Maria,  Thill,  Serge ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>September 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>[Kenny,  Eoin M.,  Ford,  Courtney,  Quinn,  M...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>May 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>[Hutter,  Frank,  Xu,  Lin,  Hoos,  Holger H.,...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>January 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>[Kohavi,  Ron,  John,  George H. ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>December 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>[Suchan,  Jakob,  Bhatt,  Mehul,  Varadarajan,...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>October 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>[Ying,  Mingsheng ]</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>February 2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  \\\n",
       "0   [Silver,  David,  Singh,  Satinder,  Precup,  ...   \n",
       "1      [Evans,  Richard,  Bošnjak,  Matko and 5 more]   \n",
       "2              [Prakken,  Henry,  Sartor,  Giovanni ]   \n",
       "3                              [Boden,  Margaret A. ]   \n",
       "4   [Lemaignan,  Séverin,  Warnier,  Mathieu and 3...   \n",
       "5                                     [Miller,  Tim ]   \n",
       "6   [Evans,  Richard,  Hernández-Orallo,  José and...   \n",
       "7   [Sharon,  Guni,  Stern,  Roni,  Felner,  Ariel...   \n",
       "8   [Sutton,  Richard S.,  Precup,  Doina,  Singh,...   \n",
       "9    [Bard,  Nolan,  Foerster,  Jakob N. and 13 more]   \n",
       "10  [van der Waa,  Jasper,  Nieuwburg,  Elisabeth,...   \n",
       "11          [Bench-Capon,  T.J.M.,  Dunne,  Paul E. ]   \n",
       "12  [Bošanský,  Branislav,  Lisý,  Viliam and 3 more]   \n",
       "13        [Luo,  Wenhan,  Xing,  Junliang and 4 more]   \n",
       "14                 [Blum,  Avrim L.,  Langley,  Pat ]   \n",
       "15              [Arora,  Saurabh,  Doshi,  Prashant ]   \n",
       "16  [Aas,  Kjersti,  Jullum,  Martin,  Løland,  An...   \n",
       "17  [Kliegr,  Tomáš,  Bahník,  Štěpán,  Fürnkranz,...   \n",
       "18  [Pereira,  Gonçalo,  Prada,  Rui,  Santos,  Pe...   \n",
       "19                 [Riveiro,  Maria,  Thill,  Serge ]   \n",
       "20  [Kenny,  Eoin M.,  Ford,  Courtney,  Quinn,  M...   \n",
       "21  [Hutter,  Frank,  Xu,  Lin,  Hoos,  Holger H.,...   \n",
       "22                 [Kohavi,  Ron,  John,  George H. ]   \n",
       "23  [Suchan,  Jakob,  Bhatt,  Mehul,  Varadarajan,...   \n",
       "24                                [Ying,  Mingsheng ]   \n",
       "\n",
       "                                        Published URL  Published Date  \n",
       "0   https://www.sciencedirect.com/science/article/...    October 2021  \n",
       "1   https://www.sciencedirect.com/science/article/...    October 2021  \n",
       "2   https://www.sciencedirect.com/science/article/...    October 2015  \n",
       "3   https://www.sciencedirect.com/science/article/...     August 1998  \n",
       "4   https://www.sciencedirect.com/science/article/...       June 2017  \n",
       "5   https://www.sciencedirect.com/science/article/...   February 2019  \n",
       "6   https://www.sciencedirect.com/science/article/...      April 2021  \n",
       "7   https://www.sciencedirect.com/science/article/...   February 2015  \n",
       "8   https://www.sciencedirect.com/science/article/...     August 1999  \n",
       "9   https://www.sciencedirect.com/science/article/...      March 2020  \n",
       "10  https://www.sciencedirect.com/science/article/...   February 2021  \n",
       "11  https://www.sciencedirect.com/science/article/...    October 2007  \n",
       "12  https://www.sciencedirect.com/science/article/...     August 2016  \n",
       "13  https://www.sciencedirect.com/science/article/...      April 2021  \n",
       "14  https://www.sciencedirect.com/science/article/...   December 1997  \n",
       "15  https://www.sciencedirect.com/science/article/...     August 2021  \n",
       "16  https://www.sciencedirect.com/science/article/...  September 2021  \n",
       "17  https://www.sciencedirect.com/science/article/...       June 2021  \n",
       "18  https://www.sciencedirect.com/science/article/...   December 2016  \n",
       "19  https://www.sciencedirect.com/science/article/...  September 2021  \n",
       "20  https://www.sciencedirect.com/science/article/...        May 2021  \n",
       "21  https://www.sciencedirect.com/science/article/...    January 2014  \n",
       "22  https://www.sciencedirect.com/science/article/...   December 1997  \n",
       "23  https://www.sciencedirect.com/science/article/...    October 2021  \n",
       "24  https://www.sciencedirect.com/science/article/...   February 2010  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 8 most downloaded articles in last 90 days\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "def journal_webscraping(url):\n",
    "    source=requests.get(url)\n",
    "    soup=BeautifulSoup(source.text,\"html.parser\")\n",
    "    \n",
    "    title=[]\n",
    "    paper_url=[]\n",
    "    authors=[]\n",
    "    publ_date=[]\n",
    "    \n",
    "    articles = soup.find('ul',class_=\"sc-9zxyh7-0 ffmPq\").find_all('li',class_=\"sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp\")\n",
    "    \n",
    "    for i in articles:\n",
    "        title.append(i.find('h2').text)\n",
    "        paper_url.append(i.find('a')['href'])\n",
    "        authors.append(i.find('span',class_='sc-1w3fpd7-0 pgLAT').text.split(','))\n",
    "        publ_date.append(i.find('span',class_='sc-1thf9ly-2 bKddwo').text)\n",
    "\n",
    "    articles_df=pd.DataFrame({'Title':title,'Authors':authors,'Published URL':paper_url,'Published Date':publ_date})\n",
    "    return articles_df\n",
    "\n",
    "journal_webscraping(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de724f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name Of Restaurant</th>\n",
       "      <th>Cuisines Served</th>\n",
       "      <th>Locaiton</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name Of Restaurant                Cuisines Served  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                   Castle Barbeque          Chinese, North Indian   \n",
       "3                        Cafe Knosh           Italian, Continental   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Locaiton Ratings  \\\n",
       "0                     Connaught Place, Central Delhi     4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                           Image URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 9 Scrape details from dineout\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "\n",
    "def dineout_scraping(url):\n",
    "       \n",
    "    source=requests.get(url)\n",
    "    data=BeautifulSoup(source.text,'html.parser')\n",
    "    \n",
    "\n",
    "    name=[]\n",
    "    location=[]\n",
    "    cuisine=[]\n",
    "    rating=[]\n",
    "    image_url=[]\n",
    "\n",
    "    for i in data.find_all('div',class_='restnt-info cursor'):\n",
    "        name.append(i.a.text)\n",
    "\n",
    "    for i in data.find_all('div',class_='restnt-loc ellipsis'):\n",
    "        location.append(i.text)\n",
    "\n",
    "    for i in data.find_all('span',class_='double-line-ellipsis'):\n",
    "        cuisine.append(i.text.split('|')[1])\n",
    "\n",
    "    for i in data.find_all('div',class_='restnt-rating rating-4'):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    for i in data.find_all('img',class_='no-img'):\n",
    "        image_url.append(i['data-src'])\n",
    "        \n",
    "    dineout_df=pd.DataFrame({'Name Of Restaurant':name,'Cuisines Served':cuisine,'Locaiton':location,'Ratings':rating,'Image URL':image_url})\n",
    "    return dineout_df\n",
    "    \n",
    "\n",
    "dineout_scraping(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8612e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5_index</th>\n",
       "      <th>h5_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication  h5_index  h5_mean\n",
       "0      1                                             Nature       444      667\n",
       "1      2                The New England Journal of Medicine       432      780\n",
       "2      3                                            Science       401      614\n",
       "3      4  IEEE/CVF Conference on Computer Vision and Pat...       389      627\n",
       "4      5                                         The Lancet       354      635\n",
       "..   ...                                                ...       ...      ...\n",
       "95    96                       Journal of Business Research       145      233\n",
       "96    97                                   Molecular Cancer       145      209\n",
       "97    98                                            Sensors       145      201\n",
       "98    99                              Nature Climate Change       144      228\n",
       "99   100                    IEEE Internet of Things Journal       144      212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 10 scrape details from google scholar\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "\n",
    "def google_webscraping(url):\n",
    "    source=requests.get(url)\n",
    "    soup=BeautifulSoup(source.text,\"html.parser\")\n",
    "    \n",
    "    publ_ds=soup.find('div',class_=\"gsc_mvt_table_wrapper\").find('td')\n",
    "    publ_ds\n",
    "    \n",
    "    rank=[]\n",
    "    publication=[]\n",
    "    h5_index=[]\n",
    "    h5_mean=[]\n",
    "    \n",
    "    for i in soup.find_all('td',class_='gsc_mvt_p'):\n",
    "        rank.append(i.text)\n",
    "    \n",
    "    for i in soup.find_all('td',class_='gsc_mvt_t'):\n",
    "        publication.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('a',class_='gs_ibl gsc_mp_anchor'):\n",
    "        h5_index.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('span',class_='gs_ibl gsc_mp_anchor'):\n",
    "        h5_mean.append(i.text)\n",
    "        \n",
    "    rank = [int(s.replace('.','')) for s in rank]\n",
    "    h5_index=[int(s) for s in h5_index]\n",
    "    h5_mean=[int(s) for s in h5_mean] \n",
    "    \n",
    "    google_publ_df=pd.DataFrame({'Rank':rank,'Publication':publication,'h5_index':h5_index,'h5_mean':h5_mean})\n",
    "    return google_publ_df \n",
    "\n",
    "google_webscraping(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
